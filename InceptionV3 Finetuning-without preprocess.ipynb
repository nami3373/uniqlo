{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', \n",
    "           '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_classes = len(classes)\n",
    "\n",
    "img_rows, img_cols = 150, 150\n",
    "channels = 3\n",
    "\n",
    "train_data_dir = 'input/processed/train'\n",
    "valid_data_dir = 'input/processed/valid'\n",
    "test_data_dir = 'input/given/test'\n",
    "\n",
    "nb_train_samples = 12399 - (15 * 24)\n",
    "nb_val_samples = 15 * 24\n",
    "nb_epoch = 30\n",
    "\n",
    "result_dir = 'results'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルと学習済み重みをロード\n",
    "# Fully-connected層（FC）はいらないのでinclude_top=False）\n",
    "input_tensor = Input(shape=(img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11799 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    color_mode='rgb',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    color_mode='rgb',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 conv2d_283\n",
      "2 batch_normalization_283\n",
      "3 activation_283\n",
      "4 conv2d_284\n",
      "5 batch_normalization_284\n",
      "6 activation_284\n",
      "7 conv2d_285\n",
      "8 batch_normalization_285\n",
      "9 activation_285\n",
      "10 max_pooling2d_13\n",
      "11 conv2d_286\n",
      "12 batch_normalization_286\n",
      "13 activation_286\n",
      "14 conv2d_287\n",
      "15 batch_normalization_287\n",
      "16 activation_287\n",
      "17 max_pooling2d_14\n",
      "18 conv2d_291\n",
      "19 batch_normalization_291\n",
      "20 activation_291\n",
      "21 conv2d_289\n",
      "22 conv2d_292\n",
      "23 batch_normalization_289\n",
      "24 batch_normalization_292\n",
      "25 activation_289\n",
      "26 activation_292\n",
      "27 average_pooling2d_28\n",
      "28 conv2d_288\n",
      "29 conv2d_290\n",
      "30 conv2d_293\n",
      "31 conv2d_294\n",
      "32 batch_normalization_288\n",
      "33 batch_normalization_290\n",
      "34 batch_normalization_293\n",
      "35 batch_normalization_294\n",
      "36 activation_288\n",
      "37 activation_290\n",
      "38 activation_293\n",
      "39 activation_294\n",
      "40 mixed0\n",
      "41 conv2d_298\n",
      "42 batch_normalization_298\n",
      "43 activation_298\n",
      "44 conv2d_296\n",
      "45 conv2d_299\n",
      "46 batch_normalization_296\n",
      "47 batch_normalization_299\n",
      "48 activation_296\n",
      "49 activation_299\n",
      "50 average_pooling2d_29\n",
      "51 conv2d_295\n",
      "52 conv2d_297\n",
      "53 conv2d_300\n",
      "54 conv2d_301\n",
      "55 batch_normalization_295\n",
      "56 batch_normalization_297\n",
      "57 batch_normalization_300\n",
      "58 batch_normalization_301\n",
      "59 activation_295\n",
      "60 activation_297\n",
      "61 activation_300\n",
      "62 activation_301\n",
      "63 mixed1\n",
      "64 conv2d_305\n",
      "65 batch_normalization_305\n",
      "66 activation_305\n",
      "67 conv2d_303\n",
      "68 conv2d_306\n",
      "69 batch_normalization_303\n",
      "70 batch_normalization_306\n",
      "71 activation_303\n",
      "72 activation_306\n",
      "73 average_pooling2d_30\n",
      "74 conv2d_302\n",
      "75 conv2d_304\n",
      "76 conv2d_307\n",
      "77 conv2d_308\n",
      "78"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ys/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., epochs=10, steps_per_epoch=376, validation_steps=360)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_302\n",
      "79 batch_normalization_304\n",
      "80 batch_normalization_307\n",
      "81 batch_normalization_308\n",
      "82 activation_302\n",
      "83 activation_304\n",
      "84 activation_307\n",
      "85 activation_308\n",
      "86 mixed2\n",
      "87 conv2d_310\n",
      "88 batch_normalization_310\n",
      "89 activation_310\n",
      "90 conv2d_311\n",
      "91 batch_normalization_311\n",
      "92 activation_311\n",
      "93 conv2d_309\n",
      "94 conv2d_312\n",
      "95 batch_normalization_309\n",
      "96 batch_normalization_312\n",
      "97 activation_309\n",
      "98 activation_312\n",
      "99 max_pooling2d_15\n",
      "100 mixed3\n",
      "101 conv2d_317\n",
      "102 batch_normalization_317\n",
      "103 activation_317\n",
      "104 conv2d_318\n",
      "105 batch_normalization_318\n",
      "106 activation_318\n",
      "107 conv2d_314\n",
      "108 conv2d_319\n",
      "109 batch_normalization_314\n",
      "110 batch_normalization_319\n",
      "111 activation_314\n",
      "112 activation_319\n",
      "113 conv2d_315\n",
      "114 conv2d_320\n",
      "115 batch_normalization_315\n",
      "116 batch_normalization_320\n",
      "117 activation_315\n",
      "118 activation_320\n",
      "119 average_pooling2d_31\n",
      "120 conv2d_313\n",
      "121 conv2d_316\n",
      "122 conv2d_321\n",
      "123 conv2d_322\n",
      "124 batch_normalization_313\n",
      "125 batch_normalization_316\n",
      "126 batch_normalization_321\n",
      "127 batch_normalization_322\n",
      "128 activation_313\n",
      "129 activation_316\n",
      "130 activation_321\n",
      "131 activation_322\n",
      "132 mixed4\n",
      "133 conv2d_327\n",
      "134 batch_normalization_327\n",
      "135 activation_327\n",
      "136 conv2d_328\n",
      "137 batch_normalization_328\n",
      "138 activation_328\n",
      "139 conv2d_324\n",
      "140 conv2d_329\n",
      "141 batch_normalization_324\n",
      "142 batch_normalization_329\n",
      "143 activation_324\n",
      "144 activation_329\n",
      "145 conv2d_325\n",
      "146 conv2d_330\n",
      "147 batch_normalization_325\n",
      "148 batch_normalization_330\n",
      "149 activation_325\n",
      "150 activation_330\n",
      "151 average_pooling2d_32\n",
      "152 conv2d_323\n",
      "153 conv2d_326\n",
      "154 conv2d_331\n",
      "155 conv2d_332\n",
      "156 batch_normalization_323\n",
      "157 batch_normalization_326\n",
      "158 batch_normalization_331\n",
      "159 batch_normalization_332\n",
      "160 activation_323\n",
      "161 activation_326\n",
      "162 activation_331\n",
      "163 activation_332\n",
      "164 mixed5\n",
      "165 conv2d_337\n",
      "166 batch_normalization_337\n",
      "167 activation_337\n",
      "168 conv2d_338\n",
      "169 batch_normalization_338\n",
      "170 activation_338\n",
      "171 conv2d_334\n",
      "172 conv2d_339\n",
      "173 batch_normalization_334\n",
      "174 batch_normalization_339\n",
      "175 activation_334\n",
      "176 activation_339\n",
      "177 conv2d_335\n",
      "178 conv2d_340\n",
      "179 batch_normalization_335\n",
      "180 batch_normalization_340\n",
      "181 activation_335\n",
      "182 activation_340\n",
      "183 average_pooling2d_33\n",
      "184 conv2d_333\n",
      "185 conv2d_336\n",
      "186 conv2d_341\n",
      "187 conv2d_342\n",
      "188 batch_normalization_333\n",
      "189 batch_normalization_336\n",
      "190 batch_normalization_341\n",
      "191 batch_normalization_342\n",
      "192 activation_333\n",
      "193 activation_336\n",
      "194 activation_341\n",
      "195 activation_342\n",
      "196 mixed6\n",
      "197 conv2d_347\n",
      "198 batch_normalization_347\n",
      "199 activation_347\n",
      "200 conv2d_348\n",
      "201 batch_normalization_348\n",
      "202 activation_348\n",
      "203 conv2d_344\n",
      "204 conv2d_349\n",
      "205 batch_normalization_344\n",
      "206 batch_normalization_349\n",
      "207 activation_344\n",
      "208 activation_349\n",
      "209 conv2d_345\n",
      "210 conv2d_350\n",
      "211 batch_normalization_345\n",
      "212 batch_normalization_350\n",
      "213 activation_345\n",
      "214 activation_350\n",
      "215 average_pooling2d_34\n",
      "216 conv2d_343\n",
      "217 conv2d_346\n",
      "218 conv2d_351\n",
      "219 conv2d_352\n",
      "220 batch_normalization_343\n",
      "221 batch_normalization_346\n",
      "222 batch_normalization_351\n",
      "223 batch_normalization_352\n",
      "224 activation_343\n",
      "225 activation_346\n",
      "226 activation_351\n",
      "227 activation_352\n",
      "228 mixed7\n",
      "229 conv2d_355\n",
      "230 batch_normalization_355\n",
      "231 activation_355\n",
      "232 conv2d_356\n",
      "233 batch_normalization_356\n",
      "234 activation_356\n",
      "235 conv2d_353\n",
      "236 conv2d_357\n",
      "237 batch_normalization_353\n",
      "238 batch_normalization_357\n",
      "239 activation_353\n",
      "240 activation_357\n",
      "241 conv2d_354\n",
      "242 conv2d_358\n",
      "243 batch_normalization_354\n",
      "244 batch_normalization_358\n",
      "245 activation_354\n",
      "246 activation_358\n",
      "247 max_pooling2d_16\n",
      "248 mixed8\n",
      "249 conv2d_363\n",
      "250 batch_normalization_363\n",
      "251 activation_363\n",
      "252 conv2d_360\n",
      "253 conv2d_364\n",
      "254 batch_normalization_360\n",
      "255 batch_normalization_364\n",
      "256 activation_360\n",
      "257 activation_364\n",
      "258 conv2d_361\n",
      "259 conv2d_362\n",
      "260 conv2d_365\n",
      "261 conv2d_366\n",
      "262 average_pooling2d_35\n",
      "263 conv2d_359\n",
      "264 batch_normalization_361\n",
      "265 batch_normalization_362\n",
      "266 batch_normalization_365\n",
      "267 batch_normalization_366\n",
      "268 conv2d_367\n",
      "269 batch_normalization_359\n",
      "270 activation_361\n",
      "271 activation_362\n",
      "272 activation_365\n",
      "273 activation_366\n",
      "274 batch_normalization_367\n",
      "275 activation_359\n",
      "276 mixed9_0\n",
      "277 concatenate_7\n",
      "278 activation_367\n",
      "279 mixed9\n",
      "280 conv2d_372\n",
      "281 batch_normalization_372\n",
      "282 activation_372\n",
      "283 conv2d_369\n",
      "284 conv2d_373\n",
      "285 batch_normalization_369\n",
      "286 batch_normalization_373\n",
      "287 activation_369\n",
      "288 activation_373\n",
      "289 conv2d_370\n",
      "290 conv2d_371\n",
      "291 conv2d_374\n",
      "292 conv2d_375\n",
      "293 average_pooling2d_36\n",
      "294 conv2d_368\n",
      "295 batch_normalization_370\n",
      "296 batch_normalization_371\n",
      "297 batch_normalization_374\n",
      "298 batch_normalization_375\n",
      "299 conv2d_376\n",
      "300 batch_normalization_368\n",
      "301 activation_370\n",
      "302 activation_371\n",
      "303 activation_374\n",
      "304 activation_375\n",
      "305 batch_normalization_376\n",
      "306 activation_368\n",
      "307 mixed9_1\n",
      "308 concatenate_8\n",
      "309 activation_376\n",
      "310 mixed10\n",
      "Epoch 1/10\n",
      "376/376 [==============================] - 5663s - loss: 2.4390 - val_loss: 3.4917\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 5717s - loss: 2.1229 - val_loss: 3.3022\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 4955s - loss: 1.9094 - val_loss: 3.1842\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 5029s - loss: 1.7682 - val_loss: 3.0636\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 5013s - loss: 1.6477 - val_loss: 2.9470\n",
      "Epoch 6/10\n",
      "376/376 [==============================] - 4972s - loss: 1.5744 - val_loss: 2.8609\n",
      "Epoch 7/10\n",
      "376/376 [==============================] - 4971s - loss: 1.5134 - val_loss: 2.7904\n",
      "Epoch 8/10\n",
      "376/376 [==============================] - 4967s - loss: 1.4539 - val_loss: 2.6921\n",
      "Epoch 9/10\n",
      "375/376 [============================>.] - ETA: 8s - loss: 1.4035 "
     ]
    }
   ],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    nb_epoch=nb_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(result_dir, 'finetuning3.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test data and make prediction\n",
    "path = os.path.join('input', 'processed', 'test', '*.jpg')\n",
    "files = sorted(glob.glob(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = []\n",
    "test = []\n",
    "\n",
    "for fl in files:\n",
    "    flbase = os.path.basename(fl)\n",
    "    img = image.load_img(fl, target_size=(img_rows, img_cols))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0\n",
    "\n",
    "    test_id.append(flbase)\n",
    "    test.append(model.predict(x)[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_records(test, index=test_id)\n",
    "df1.to_csv('sub1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_list = []\n",
    "df1_T = df1.transpose()\n",
    "for i in range(len(df1_T.columns)):\n",
    "    label_list.append(df1_T.ix[:, i].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.Series(label_list, index=df1.index)\n",
    "df2.to_csv('sub2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('sub1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('sub2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
